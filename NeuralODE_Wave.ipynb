{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32675,"status":"ok","timestamp":1688573261605,"user":{"displayName":"Luke McLennan","userId":"05318381855726906118"},"user_tz":300},"id":"1FXYOspA13S2","outputId":"a4bec678-e0ef-4f00-b86c-918113387c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchdiffeq\n","  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.0.1+cu118)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->torchdiffeq) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n","Installing collected packages: torchdiffeq\n","Successfully installed torchdiffeq-0.2.3\n","Mounted at /content/drive\n"]}],"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n","from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n","!pip install torchdiffeq\n","from torchdiffeq import odeint_adjoint, odeint\n","import numpy as np\n","from scipy import io\n","import argparse\n","import os\n","import pickle\n","import matplotlib.pyplot as plt\n","from timeit import default_timer\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mQJSSkq2Hal"},"outputs":[],"source":["directory = \"drive/MyDrive/\"#\"spring-runs_train/\" #/mnt/data/datasets/physical_system_benchmark/experiment_output/spring/spring/run/data_gen/spring-runs_train-spring-n1000-t805-n0_00001/\"\n","npz = np.load(directory + \"trajectories_wave.npz\")\n","nb_trajs = 50\n","start_traj = 0\n","nb_times = 10205\n","x_width = 125\n","p1, q1, dqdt1, dpdt1, t1 = np.zeros((nb_trajs,x_width,nb_times)), np.zeros((nb_trajs,x_width,nb_times)), np.zeros((nb_trajs,x_width,nb_times)), np.zeros((nb_trajs,x_width,nb_times)), np.zeros((nb_trajs,x_width,nb_times))\n","for i in range(start_traj, start_traj + nb_trajs):\n","  p1[i,:,:], q1[i,:,:], dqdt1[i,:,:], dpdt1[i,:,:], t1[i,:,:] = npz[npz.files[5*i+0]].T, npz[npz.files[5*i+1]].T, npz[npz.files[5*i+2]].T, npz[npz.files[5*i+3]].T, npz[npz.files[5*i+4]].T\n","q_exact = torch.tensor(q1, dtype = torch.float32)\n","p_exact = torch.tensor(p1, dtype = torch.float32)\n","t = torch.tensor(t1[0], dtype = torch.float32)\n","exact = torch.cat((q_exact.reshape(nb_trajs,x_width,nb_times,1), p_exact.reshape(nb_trajs,x_width,nb_times,1)), dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPSNy7oqItdz"},"outputs":[],"source":["test_trajs = 16\n","train_loader = DataLoader(exact[:(nb_trajs-test_trajs)], batch_size=16)\n","test_loader  = DataLoader(exact[(nb_trajs-test_trajs):], batch_size=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1688573311073,"user":{"displayName":"Luke McLennan","userId":"05318381855726906118"},"user_tz":300},"id":"2fELOSgY6ncA","outputId":"f4592cec-dca1-4609-92b5-d3ff63667fbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([50, 125, 10205, 2])\n","16\n"]}],"source":["print(exact.shape)\n","print(len(exact[(nb_trajs-test_trajs):]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mj5tWMTN2Llx"},"outputs":[],"source":["class Lyapunov(nn.Module):\n","  def __init__(self, in_nodes, layers, activation):\n","    super(Lyapunov, self).__init__()\n","    self.in_nodes = in_nodes\n","    self.layers = layers\n","    self.act = activation\n","    self.w0 = nn.Linear(in_nodes, in_nodes)\n","    self.w1 = nn.Linear(in_nodes, 1)\n","    self.u1 = nn.Parameter(torch.Tensor(1, in_nodes))\n","    nn.init.xavier_uniform_(self.u1)\n","    self.g0 = self.g(torch.zeros(in_nodes))\n","  def g(self, x):\n","    z = self.act(self.w0(x))\n","    z = self.act(self.w1(x)  + F.linear(z, self.u1.exp()))\n","    return z\n","  def forward(self, x):\n","    return self.act(self.g(x) - self.g0) + torch.sum(x**2, dim=1).reshape(-1,1)/100\n","# class Lyapunov(nn.Module):\n","#   def __init__(self, in_nodes, layers, activation):\n","#     super(Lyapunov, self).__init__()\n","#     self.in_nodes = in_nodes\n","#     self.layers = layers\n","#     self.act = activation\n","#     self.W = [nn.Linear(in_nodes, in_nodes)]\n","#     self.U = []\n","#     for i in range(layers):\n","#       self.W.append(nn.Linear(in_nodes, in_nodes))\n","#       self.U.append(nn.Linear(in_nodes, in_nodes))\n","#     self.W.append(nn.Linear(in_nodes, 1))\n","#     self.U.append(nn.Linear(in_nodes, 1))\n","#     self.g0 = self.g(torch.zeros(in_nodes))\n","#   def g(self, x):\n","#     z = self.act(self.W[0](x))\n","#     for i in range(self.layers):\n","#       z = self.act(self.W[i+1](x) + self.U[i](z)**2)\n","#     z = self.act(self.W[self.layers+1](x) + self.U[self.layers](z)**2)\n","#     return z\n","#   def forward(self, x):\n","#     return self.act(self.g(x) - self.g0) + torch.norm(x, p=2)/100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpRguIif2RBg"},"outputs":[],"source":["class ODEFunc(nn.Module):\n","    def __init__(self, in_nodes, hid_nodes, v_layers, v_act):\n","        super(ODEFunc, self).__init__()\n","        self.fc0 = nn.Linear(in_nodes+1, hid_nodes)\n","        self.fc1 = nn.Linear(hid_nodes, in_nodes)\n","        self.V = Lyapunov(in_nodes, v_layers, v_act)\n","    def forward(self, t, x):\n","        x = x.requires_grad_(True)\n","        tt = torch.ones_like(x[...,:1], requires_grad=True)*t\n","        xt = torch.cat((tt, x), dim=-1)\n","        fx = self.fc0(xt)\n","        fx = F.gelu(fx)\n","        fx = self.fc1(fx)\n","        # Vx = self.V(x)\n","        # grad_Vx = torch.autograd.grad(Vx, x, torch.ones(Vx.shape), create_graph=True, allow_unused=True, retain_graph=True)[0]\n","        # alpha = 1\n","        # fx = fx - grad_Vx*torch.relu(torch.sum(grad_Vx*fx,dim=1).reshape(-1,1) + alpha*Vx)/torch.sum(grad_Vx**2, dim=1).reshape(-1,1)\n","        return fx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSz7AWZK2_XU"},"outputs":[],"source":["class NODE(nn.Module):\n","    def __init__(self, in_nodes, hid_nodes, tol, v_layers, v_act):\n","        super(NODE, self).__init__()\n","        self.ode = ODEFunc(in_nodes, hid_nodes, v_layers, v_act)\n","        self.fc0 = nn.Linear(2, in_nodes)\n","        self.fc1 = nn.Linear(in_nodes, 2)\n","        self.tol = tol\n","    def forward(self, x):\n","        x = x.requires_grad_(True)\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.fc0(x)\n","        x0 = x\n","        x = odeint(self.ode, x, torch.tensor([0., 1.], device=x.device), atol=self.tol, rtol=self.tol)[-1]\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7403,"status":"ok","timestamp":1688573318691,"user":{"displayName":"Luke McLennan","userId":"05318381855726906118"},"user_tz":300},"id":"yZarwrnG3BwW","outputId":"453b4252-e75e-492d-892c-ccb9bdf65aa5"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["skip_size = 157\n","skip_size_2 = 5\n","Nt = nb_times//skip_size\n","Nx = x_width//skip_size_2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","# model = NODE(32, 128, 1e-5, 3, nn.Softplus()).to(device)\n","latent_channels = 32\n","hidden_layers = 64\n","odefunc = ODEFunc(latent_channels, hidden_layers, 2, nn.ReLU())\n","prelayer = nn.Linear(2*Nx, latent_channels)\n","postlayer = nn.Linear(latent_channels, 2*Nx)\n","params = list(odefunc.parameters()) + list(prelayer.parameters()) + list(postlayer.parameters())\n","optim = torch.optim.Adam(params, lr=1e-3)\n","loss_fn = nn.MSELoss()\n","tol = 1e-5\n","mse_train_losses = []\n","mse_test_losses = []\n","lyap_train_losses = []\n","lyap_test_losses = []\n","accuracies = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeAig_Mfp46z"},"outputs":[],"source":["epochs = 500"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QwyZAkWX3F1u","executionInfo":{"status":"error","timestamp":1688575226423,"user_tz":300,"elapsed":1907746,"user":{"displayName":"Luke McLennan","userId":"05318381855726906118"}},"outputId":"1c57de00-d1e4-4089-a536-ae46d410d0f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 32.03916271 0.08695312092701594 0.0 0.08695312092701594 0.08660335838794708\n","10 3.1526202329999933 0.055304585645596184 0.0 0.055304585645596184 0.05766931176185608\n","20 4.491112035000015 0.0429945799211661 0.0 0.0429945799211661 0.045000120997428894\n","30 2.984661041000038 0.03870696077744166 0.0 0.03870696077744166 0.04049089923501015\n","40 3.0643729249999865 0.03162442892789841 0.0 0.03162442892789841 0.03279230371117592\n","50 3.551984499000014 0.02804003283381462 0.0 0.02804003283381462 0.029108328744769096\n","60 2.9550126949999935 0.024398093422253925 0.0 0.024398093422253925 0.02493923157453537\n","70 4.854171941999994 0.018641235306859016 0.0 0.018641235306859016 0.01881839521229267\n","80 3.175408699000002 0.015512126497924328 0.0 0.015512126497924328 0.01552574522793293\n","90 3.2169265329999917 0.013246201599637667 0.0 0.013246201599637667 0.013088085688650608\n","100 3.164024970000014 0.010806121242543062 0.0 0.010806121242543062 0.010479848831892014\n","110 3.0943831490000093 0.009158027979234854 0.0 0.009158027979234854 0.00881792139261961\n","120 3.1745356999999785 0.008484231463323036 0.0 0.008484231463323036 0.008181706070899963\n","130 3.294101684999987 0.007895421391973892 0.0 0.007895421391973892 0.007511836010962725\n","140 3.306682952000074 0.007293421309441328 0.0 0.007293421309441328 0.006845021620392799\n","150 3.3019951329999913 0.006708410878976186 0.0 0.006708410878976186 0.006212017964571714\n","160 4.054790349999962 0.006222753630330165 0.0 0.006222753630330165 0.005716896150261164\n","170 2.982598087000042 0.005879940930753946 0.0 0.005879940930753946 0.005396862514317036\n","180 4.059248163999996 0.005558636349936326 0.0 0.005558636349936326 0.005101057700812817\n","190 3.0260491540000203 0.00535264906163017 0.0 0.00535264906163017 0.004800839815288782\n","200 4.261071068999968 0.005143353405098121 0.0 0.005143353405098121 0.004618349485099316\n","210 3.1112736210000094 0.004989896124849717 0.0 0.004989896124849717 0.004472228232771158\n","220 4.415500828000063 0.004910952722032865 0.0 0.004910952722032865 0.004514027386903763\n","230 3.268450250000001 0.004628752823919058 0.0 0.004628752823919058 0.0041068438440561295\n","240 3.5366423830000713 0.004530752543359995 0.0 0.004530752543359995 0.004007008392363787\n","250 4.562114533999875 0.004337033877770106 0.0 0.004337033877770106 0.003786493092775345\n","260 3.314410041999963 0.0042397974369426565 0.0 0.0042397974369426565 0.0036272427532821894\n","270 4.232684261000031 0.004049059314032395 0.0 0.004049059314032395 0.0035024790558964014\n","280 3.372001366000177 0.0038539638432363668 0.0 0.0038539638432363668 0.003371000522747636\n","290 3.2233119879999776 0.003670804280166825 0.0 0.003670804280166825 0.0031581169459968805\n","300 3.2401155870002185 0.003528720000758767 0.0 0.003528720000758767 0.003027823753654957\n","310 3.2193607429999247 0.003481584290663401 0.0 0.003481584290663401 0.0029232800006866455\n","320 3.140843084999915 0.003303688019514084 0.0 0.003303688019514084 0.002821853384375572\n","330 3.7875139179998314 0.003162108361721039 0.0 0.003162108361721039 0.0026893524918705225\n","340 3.246408016000032 0.0031677202011148133 0.0 0.0031677202011148133 0.002693578600883484\n","350 3.9651918100000785 0.003109477382774154 0.0 0.003109477382774154 0.002536311512812972\n","360 3.2847545670001637 0.00344906405856212 0.0 0.00344906405856212 0.0026783987414091825\n","370 4.340112353999984 0.0028663127838323512 0.0 0.0028663127838323512 0.002411595778539777\n","380 3.1845109349999348 0.0027881186300267777 0.0 0.0027881186300267777 0.002353645395487547\n","390 4.393403546000172 0.0027265511453151703 0.0 0.0027265511453151703 0.0022984545212239027\n","400 3.112074163999978 0.002679167160143455 0.0 0.002679167160143455 0.002246277639642358\n","410 4.246526236000136 0.002722537455459436 0.0 0.002722537455459436 0.002207383280619979\n","420 3.197498038000049 0.0026552649990965924 0.0 0.0026552649990965924 0.0022144836839288473\n","430 4.02522402999989 0.002703758810336391 0.0 0.002703758810336391 0.0021573540288954973\n","440 3.2100501709999207 0.0025651621787498393 0.0 0.0025651621787498393 0.0021450070198625326\n","450 3.4522214749999875 0.002463424578309059 0.0 0.002463424578309059 0.0020757396705448627\n","460 3.483462502000066 0.002422924308727185 0.0 0.002422924308727185 0.002021934138610959\n","470 3.24639060100003 0.0024159400102992854 0.0 0.0024159400102992854 0.001987788127735257\n","480 3.22446789799983 0.0026210883321861425 0.0 0.0026210883321861425 0.0023505650460720062\n","490 3.3216454549999526 0.0023479461669921875 0.0 0.0023479461669921875 0.0019813370890915394\n","500 3.6958923949998734 0.002450693864375353 0.0 0.002450693864375353 0.002187405712902546\n","510 3.2171744579998176 0.0022581126540899277 0.0 0.0022581126540899277 0.001875704270787537\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-ce0acb833064>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#   loss = loss + torch.relu(loss_now + (alpha-1)*loss_prev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# for k,p in enumerate(params):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#   if not grads_added:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                \"of them.\")\n\u001b[1;32m    273\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/adjoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *grad_y)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# Run the augmented system backwards in time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 aug_state = odeint(\n\u001b[0m\u001b[1;32m    127\u001b[0m                     \u001b[0maugmented_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# trigger both. (i.e. interleaving them would be wrong.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# dtypes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# y1.dtype == self.y0.dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UncheckedAssign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["alpha = 5\n","grads = []\n","grads_added = False\n","filter = []\n","for i in range(epochs):\n","    t = default_timer()\n","    lyap_train_loss = 0\n","    mse_train_loss = 0\n","    mse_test_loss = 0\n","    lyap_test_loss = 0\n","    for x in train_loader:\n","        x = x[:,::skip_size_2,:,:]\n","        x0 = x[:,:,0,:].to(device).requires_grad_(True)\n","        x0 = x0.reshape(x.shape[0], -1)\n","        z0 = prelayer(x0)\n","        zt = odeint_adjoint(odefunc, z0, torch.linspace(0,1,Nt), atol=tol, rtol=tol)\n","        pred = postlayer(zt)\n","        x = x[:,:,::skip_size,:].permute(2, 0, 1, 3)\n","        pred = pred.reshape(x.shape)\n","        # print(pred.shape, x.shape)\n","        # pred = pred.permute(2, 0, 1, 3)\n","        loss = loss_fn(pred,x)\n","        # loss = torch.tensor(0.0, requires_grad=True, device=device)\n","        # loss_now = loss_fn(pred[0], x[0])\n","        # for j in range(1,pred.shape[0]):\n","        #   loss_prev = loss_now\n","        #   loss_now = loss_fn(pred[j], x[j])\n","        #   loss = loss + torch.relu(loss_now + (alpha-1)*loss_prev)\n","        optim.zero_grad()\n","        loss.backward()\n","        # for k,p in enumerate(params):\n","        #   if not grads_added:\n","        #     grads.append(torch.abs(p.grad))\n","        #     filter.append(torch.ones(p.grad.shape))\n","        #   else:\n","        #     grads[k] = grads[k] + torch.abs(p.grad)\n","        # grads_added = True\n","        # for j, p in enumerate(params):\n","        #   p.grad = p.grad*filter[j]\n","        optim.step()\n","        lyap_train_loss += loss.item()\n","        mse_train_loss += loss_fn(pred, x).item()\n","    for x in test_loader:\n","        x = x[:,::skip_size_2,:,:]\n","        x = x[:,:,::skip_size,:]\n","        x0 = x[:,:,0,:].to(device).requires_grad_(True)\n","        x0 = x0.reshape(x.shape[0], -1)\n","        z0 = prelayer(x0)\n","        zt = odeint_adjoint(odefunc, z0, torch.linspace(0,1,Nt), atol=tol, rtol=tol)\n","        pred = postlayer(zt)\n","        x = x.permute(2, 0, 1, 3)\n","        pred = pred.reshape(x.shape)\n","        # pred = pred.permute(2, 0, 1, 3)\n","        loss = torch.tensor(0.0, requires_grad=True, device=device)\n","        # loss_now = loss_fn(pred[0], x[0])\n","        # for j in range(1,pred.shape[0]):\n","        #   loss_prev = loss_now\n","        #   loss_now = loss_fn(pred[j], x[j])\n","        #   loss = loss + torch.relu(loss_now + (alpha-1)*loss_prev)\n","        lyap_test_loss += loss.item()\n","        mse_test_loss += loss_fn(pred, x).item()\n","    mse_train_loss /= len(train_loader)\n","    mse_test_loss /= len(test_loader)\n","    lyap_train_loss /= len(train_loader)\n","    lyap_test_loss /= len(test_loader)\n","    mse_train_losses.append(mse_train_loss)\n","    mse_test_losses.append(mse_test_loss)\n","    lyap_train_losses.append(lyap_train_loss)\n","    lyap_test_losses.append(lyap_test_loss)\n","    if i%10==0 or i ==epochs-1:\n","      print(i, default_timer()-t, lyap_train_loss, lyap_test_loss, mse_train_loss, mse_test_loss)\n","      # for p,g,f in zip(params,grads,filter):\n","      #   j = torch.argmax(f/g, keepdim=True)\n","      #   # print(j, g.flatten()[j])\n","      #   f2 = f.flatten()\n","      #   f2[j] = 0\n","      #   f = f2.reshape(f.shape)\n","        # print(p[j//p.shape[1],j%p.shape[1]])\n","        # p[j//p.shape[1],j%p.shape[1]].requires_grad = False\n","        # p.requires_grad=False\n","        # print(j)\n","        # print(p.shape, g.shape)\n","        # print(p.flatten()[j], g.flatten()[j])\n","# for g in grads:\n","#   print(g)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BVJpLHUYRpn"},"outputs":[],"source":["torch.save([lyap_train_losses[:500], lyap_test_losses[:500], mse_train_losses[:500], mse_test_losses[:500]], \"node_wave_losses.data\")"]},{"cell_type":"code","source":["torch.save([prelayer.state_dict(), odefunc.state_dict(), postlayer.state_dict()], \"node_wave_model.data\")"],"metadata":{"id":"zZ9bglub7tO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWMdV7LnZwxk"},"outputs":[],"source":["plt.plot(range(len(mse_train_losses)), np.array(mse_train_losses), label=\"MSE train loss\")\n","plt.plot(range(len(mse_test_losses)), np.array(mse_test_losses), label=\"MSE test loss\")\n","plt.yscale('log')\n","plt.legend()\n","plt.show()\n","plt.plot(range(len(lyap_train_losses)), np.array(lyap_train_losses), label=\"Lyap. train loss\")\n","plt.plot(range(len(lyap_test_losses)), np.array(lyap_test_losses), label=\"Lyap. test loss\")\n","plt.yscale('log')\n","plt.legend()\n","plt.show()\n","plt.plot(range(len(mse_test_losses)), np.abs(np.array(mse_test_losses)-np.array(mse_train_losses)), label=\"MSE difference\")\n","plt.yscale('log')\n","plt.legend()\n","plt.show()\n","plt.plot(range(len(lyap_test_losses)), np.abs(np.array(lyap_test_losses)-np.array(lyap_train_losses)), label=\"Lyap. difference\")\n","plt.legend()\n","plt.yscale('log')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1oEUVEX12S7CTukeUyzA_w8Y1UPuy824m","timestamp":1688326203517},{"file_id":"19btNNzNCoRkktc87sqJDWQ6LY47GETdo","timestamp":1688156252218},{"file_id":"1Y4UoIhAYn-IDWHjNdx6Zpj2LcOJXxxu3","timestamp":1687663152617},{"file_id":"1uoRiiJxJNvycS1YYSHBLvkfbEigQePTh","timestamp":1687295965944}],"authorship_tag":"ABX9TyMzXRhWT+1KdoJeGyO3Ef98"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}